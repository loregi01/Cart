{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfxaZ2c0nQI3ADHsfmgsHf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/loregi01/Cart/blob/main/Cart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nGkuAmbyIjz-"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import Env\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define the two policies we will use, the Random policy for the first attempt and the Learning Policy for the second attempt."
      ],
      "metadata": {
        "id": "-gZeAQTCPvbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomPolicy:\n",
        "    def __init__(self, n_actions):\n",
        "        self.n_actions = n_actions\n",
        "    def __call__(self, obs) -> int:\n",
        "        return random.randint(0, self.n_actions - 1)\n",
        "\n",
        "class GreedyPolicy:\n",
        "    def __init__(self, Q):\n",
        "        self.Q = Q\n",
        "    def __call__(self, obs) -> int:\n",
        "        return np.argmax(self.Q[obs])\n",
        "\n",
        "class LearningPolicy:\n",
        "    def __init__(self, Q):\n",
        "        self.Q = Q\n",
        "        self.n_actions = len(Q[0])\n",
        "    def __call__(self, obs, eps: float) -> int:\n",
        "        greedy = random.random() > eps\n",
        "        if greedy:\n",
        "            return np.argmax(self.Q[obs])\n",
        "        else:\n",
        "            return random.randint(0, self.n_actions - 1)"
      ],
      "metadata": {
        "id": "KL-FOBFyPvB_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "First we try the environment using the random policy, this means that we don't use learning but we perform randomly a new action at each time"
      ],
      "metadata": {
        "id": "anIX6JvfMK8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can define a function for the first attempt"
      ],
      "metadata": {
        "id": "7O4PbSr2QfJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_attempt (env: gym.Env, policy, discount_factor: float, n_episodes: int, render=False) -> float:\n",
        "\n",
        "  sum = 0.0\n",
        "  observation = env.reset()\n",
        "  discounting = 1\n",
        "\n",
        "  for episode in range(0, n_episodes):\n",
        "\n",
        "    discounting = 1\n",
        "\n",
        "    print (\"Episode: \" + str(episode))\n",
        "\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done :\n",
        "      if done:\n",
        "        observation = env.reset()\n",
        "        discounting = 1\n",
        "      obs = observation[0] + observation[1]\n",
        "\n",
        "      selected_action = policy(obs)\n",
        "\n",
        "      # let's analyze the effects of our actions\n",
        "      # again we can ignore the info parameter\n",
        "      observation, reward, done, info = env.step(selected_action)\n",
        "\n",
        "      sum += reward * discounting\n",
        "      discounting *= discount_factor\n",
        "\n",
        "  return sum/n_episodes"
      ],
      "metadata": {
        "id": "PzfKy1eLQdFe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "n_episodes = 20000\n",
        "start_epsilon = 1.0\n",
        "epsilon_decay = start_epsilon / (n_episodes / 2)\n",
        "final_epsilon = 0.1\n",
        "eps = start_epsilon\n",
        "discount_factor = 0.95\n",
        "\n",
        "env = gym.make('MountainCar-v0')\n",
        "\n",
        "random_trial = random_attempt (env, RandomPolicy(env.action_space.n), discount_factor, n_episodes, render = True)\n",
        "\n",
        "print(random_trial)"
      ],
      "metadata": {
        "id": "em0p94_UMHrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def learning_attempt (env: gym.Env, learning_rate: float, discount_factor: float, n_episodes: int):\n",
        "\n",
        "  Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "  policy = LearningPolicy(Q)\n",
        "\n",
        "  for episode in range(0, n_episodes):\n",
        "    observation, info = env.reset()\n",
        "    done = False\n",
        "    flag = True\n",
        "    flag2 = True\n",
        "    eps = (n_episodes - episode) / n_episodes\n",
        "\n",
        "    while not done :\n",
        "      if done:\n",
        "          observation = env.reset()\n",
        "\n",
        "      if flag2:\n",
        "        selected_action = policy(observation, eps)\n",
        "        flag2 = False\n",
        "      else:\n",
        "        selected_action = policy(observation[0]+observation[1], eps)\n",
        "\n",
        "      new_observation, reward, done, info = env.step(selected_action)\n",
        "\n",
        "      if flag:\n",
        "        obs1 = observation\n",
        "        obs2 = new_observation[0] + new_observation [1]\n",
        "        flag = False\n",
        "      else:\n",
        "        obs1 = observation[0] + observation[1]\n",
        "        obs2 = new_observation[0] + new_observation [1]\n",
        "\n",
        "      Q[obs1][selected_action] += learning_rate * (reward + discount_factor * np.max(Q[obs2]) - Q[obs1][selected_action])\n",
        "\n",
        "      observation = new_observation\n",
        "    eps = max(final_epsilon, eps - epsilon_decay)\n",
        "\n",
        "  return Q"
      ],
      "metadata": {
        "id": "bImjYpHBZVMN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_table = learning_attempt (env, learning_rate, discount_factor, n_episodes)"
      ],
      "metadata": {
        "id": "-bWnkKVSYigh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training = random_attempt(env, GreedyPolicy(q_table), learning_rate, n_episodes, render = True)\n",
        "print(training)"
      ],
      "metadata": {
        "id": "BIBg6UUPpbi6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}